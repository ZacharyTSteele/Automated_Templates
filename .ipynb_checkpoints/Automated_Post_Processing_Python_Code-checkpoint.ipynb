{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abcdf9b",
   "metadata": {},
   "source": [
    "Load in relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf551078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics #Import statistics package\n",
    "import pandas as pd #Import pandas as pd\n",
    "import numpy as np #Import pandas as np\n",
    "from math import exp #import exp for equations \n",
    "import sys #import sys for defining spaces later\n",
    "import time #import time for setting date \n",
    "timestr = time.strftime(\"_%d_%Y_%B\") #Set date in day_Year_Month format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9efbe",
   "metadata": {},
   "source": [
    "Below established and known values should be imported using a Values.py file for comparison with measured values from the isotope analyzer instrument. The international standard values are fixed (e.g., VSMOW and SLAP), but the in-house standards and control waters you use within your own lab will chage over time as you generate more measurments. The .py file should consistently be updated over time as new measurements are made during Standards runs.\n",
    "\n",
    "Examples of in-house waters included in this file would be MegaBoil and MissMT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7347aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Values2 import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cbed0",
   "metadata": {},
   "source": [
    "Now load in raw standards data from the CRDS instrument\n",
    "\n",
    "*** EDIT BASED ON LOCATION OF THE FILE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c974e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"..\\\\Automated_Templates\\\\Test_Data.csv\",sep=',')\n",
    "# Picarro Data from Standards run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46de38a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['  Line', '  Analysis', '             Time Code', '           Port',\n",
       "       '  Inj Nr', '  d(17_16)Mean', '  d(18_16)Mean', '    d(D_H)Mean',\n",
       "       '      E17_Mean', '      H2O_Mean', '  Ignore', ' Good',\n",
       "       '                            Identifier 1',\n",
       "       '                            Identifier 2', '   Gas Configuration',\n",
       "       'Timestamp Mean', '   d(17_16)_SD', '   d(18_16)_SD', '     d(D_H)_SD',\n",
       "       '        E17_SD', '        H2O_SD', '   d(18_16)_Sl', '     d(D_H)_Sl',\n",
       "       '        H2O_Sl', 'baseline_shift', '   slope_shift', '     residuals',\n",
       "       'baseline_curvature', '      interval', '       ch4_ppm',\n",
       "       '  h16od_adjust', '   h16od_shift', 'n2_flag', 'Resistance',\n",
       "       '      DAS Temp', '      Tray', '  Sample', '     Job', '    Method',\n",
       "       'Error Code', 'Pulse Good'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.columns #load column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475a58f",
   "metadata": {},
   "source": [
    "The raw data from a the CRDS instrument (Picarro) has a lot of weird spacing issues that need to be cleaned up to make the process easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff74ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.columns=Data.columns.str.strip() #Remove all blank spaces from the column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb271b",
   "metadata": {},
   "source": [
    "Now revisit the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c9a7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Line', 'Analysis', 'Time Code', 'Port', 'Inj Nr', 'd(17_16)Mean',\n",
       "       'd(18_16)Mean', 'd(D_H)Mean', 'E17_Mean', 'H2O_Mean', 'Ignore', 'Good',\n",
       "       'Identifier 1', 'Identifier 2', 'Gas Configuration', 'Timestamp Mean',\n",
       "       'd(17_16)_SD', 'd(18_16)_SD', 'd(D_H)_SD', 'E17_SD', 'H2O_SD',\n",
       "       'd(18_16)_Sl', 'd(D_H)_Sl', 'H2O_Sl', 'baseline_shift', 'slope_shift',\n",
       "       'residuals', 'baseline_curvature', 'interval', 'ch4_ppm',\n",
       "       'h16od_adjust', 'h16od_shift', 'n2_flag', 'Resistance', 'DAS Temp',\n",
       "       'Tray', 'Sample', 'Job', 'Method', 'Error Code', 'Pulse Good'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.columns #load column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec156c7c",
   "metadata": {},
   "source": [
    "Now the column names are clean which will make adjusting the data later in the process much easier. \n",
    "\n",
    "Now look at the data types for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba999cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 681 entries, 0 to 680\n",
      "Data columns (total 41 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Line                681 non-null    int64  \n",
      " 1   Analysis            681 non-null    object \n",
      " 2   Time Code           681 non-null    object \n",
      " 3   Port                681 non-null    object \n",
      " 4   Inj Nr              681 non-null    int64  \n",
      " 5   d(17_16)Mean        681 non-null    float64\n",
      " 6   d(18_16)Mean        681 non-null    float64\n",
      " 7   d(D_H)Mean          681 non-null    float64\n",
      " 8   E17_Mean            681 non-null    float64\n",
      " 9   H2O_Mean            681 non-null    int64  \n",
      " 10  Ignore              681 non-null    int64  \n",
      " 11  Good                681 non-null    int64  \n",
      " 12  Identifier 1        681 non-null    object \n",
      " 13  Identifier 2        681 non-null    object \n",
      " 14  Gas Configuration   681 non-null    object \n",
      " 15  Timestamp Mean      681 non-null    int64  \n",
      " 16  d(17_16)_SD         681 non-null    float64\n",
      " 17  d(18_16)_SD         681 non-null    float64\n",
      " 18  d(D_H)_SD           681 non-null    float64\n",
      " 19  E17_SD              681 non-null    float64\n",
      " 20  H2O_SD              681 non-null    float64\n",
      " 21  d(18_16)_Sl         681 non-null    float64\n",
      " 22  d(D_H)_Sl           681 non-null    float64\n",
      " 23  H2O_Sl              681 non-null    float64\n",
      " 24  baseline_shift      681 non-null    float64\n",
      " 25  slope_shift         681 non-null    float64\n",
      " 26  residuals           681 non-null    float64\n",
      " 27  baseline_curvature  681 non-null    float64\n",
      " 28  interval            681 non-null    float64\n",
      " 29  ch4_ppm             681 non-null    float64\n",
      " 30  h16od_adjust        681 non-null    int64  \n",
      " 31  h16od_shift         681 non-null    int64  \n",
      " 32  n2_flag             681 non-null    int64  \n",
      " 33  Resistance          681 non-null    float64\n",
      " 34  DAS Temp            681 non-null    float64\n",
      " 35  Tray                681 non-null    int64  \n",
      " 36  Sample              681 non-null    int64  \n",
      " 37  Job                 681 non-null    int64  \n",
      " 38  Method              681 non-null    object \n",
      " 39  Error Code          681 non-null    int64  \n",
      " 40  Pulse Good          681 non-null    int64  \n",
      "dtypes: float64(20), int64(14), object(7)\n",
      "memory usage: 218.3+ KB\n"
     ]
    }
   ],
   "source": [
    "Data.info() #load the data type for each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfff6c2",
   "metadata": {},
   "source": [
    "*** If data reads in the 'd(18_16)Mean', 'd(17_16)Mean', 'd(D_H)Mean', 'E17_Mean', and 'H2O_Mean' variables as non-intergers and with weird spaces, use the next two sections of code. Otherwise, section 8 will return an error and just proceed to section 10 ***\n",
    "\n",
    "Because of the weird formating of the raw data, Python is having a tough time reading the data correctly. Remove the spacing in the variables of interest and convert to numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05fa308",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21240\\3843258171.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd(18_16)Mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd(18_16)Mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#removing spacing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd(17_16)Mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd(17_16)Mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#removing spacing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd(D_H)Mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd(D_H)Mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#removing spacing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'E17_Mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'E17_Mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#removing spacing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'H2O_Mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'H2O_Mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#removing spacing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5573\u001b[0m         ):\n\u001b[0;32m   5574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5575\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can only use .str accessor with string values!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "Data['d(18_16)Mean'] = Data['d(18_16)Mean'].str.strip() #removing spacing\n",
    "Data['d(17_16)Mean'] = Data['d(17_16)Mean'].str.strip() #removing spacing\n",
    "Data['d(D_H)Mean'] = Data['d(D_H)Mean'].str.strip() #removing spacing\n",
    "Data['E17_Mean'] = Data['E17_Mean'].str.strip() #removing spacing\n",
    "Data['H2O_Mean'] = Data['H2O_Mean'].str.strip() #removing spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea6af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['d(18_16)Mean'] = pd.to_numeric(Data['d(18_16)Mean']) #convert to numeric\n",
    "Data['d(17_16)Mean'] = pd.to_numeric(Data['d(17_16)Mean']) #convert to numeric\n",
    "Data['d(D_H)Mean'] = pd.to_numeric(Data['d(D_H)Mean']) #convert to numeric\n",
    "Data['E17_Mean'] = pd.to_numeric(Data['E17_Mean']) #convert to numeric\n",
    "Data['H2O_Mean'] = pd.to_numeric(Data['H2O_Mean']) #convert to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241fcd2e",
   "metadata": {},
   "source": [
    "Now lets take another look at the data types for the variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a7e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 681 entries, 0 to 680\n",
      "Data columns (total 41 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Line                681 non-null    int64  \n",
      " 1   Analysis            681 non-null    object \n",
      " 2   Time Code           681 non-null    object \n",
      " 3   Port                681 non-null    object \n",
      " 4   Inj Nr              681 non-null    int64  \n",
      " 5   d(17_16)Mean        681 non-null    float64\n",
      " 6   d(18_16)Mean        681 non-null    float64\n",
      " 7   d(D_H)Mean          681 non-null    float64\n",
      " 8   E17_Mean            681 non-null    float64\n",
      " 9   H2O_Mean            681 non-null    int64  \n",
      " 10  Ignore              681 non-null    int64  \n",
      " 11  Good                681 non-null    int64  \n",
      " 12  Identifier 1        681 non-null    object \n",
      " 13  Identifier 2        681 non-null    object \n",
      " 14  Gas Configuration   681 non-null    object \n",
      " 15  Timestamp Mean      681 non-null    int64  \n",
      " 16  d(17_16)_SD         681 non-null    float64\n",
      " 17  d(18_16)_SD         681 non-null    float64\n",
      " 18  d(D_H)_SD           681 non-null    float64\n",
      " 19  E17_SD              681 non-null    float64\n",
      " 20  H2O_SD              681 non-null    float64\n",
      " 21  d(18_16)_Sl         681 non-null    float64\n",
      " 22  d(D_H)_Sl           681 non-null    float64\n",
      " 23  H2O_Sl              681 non-null    float64\n",
      " 24  baseline_shift      681 non-null    float64\n",
      " 25  slope_shift         681 non-null    float64\n",
      " 26  residuals           681 non-null    float64\n",
      " 27  baseline_curvature  681 non-null    float64\n",
      " 28  interval            681 non-null    float64\n",
      " 29  ch4_ppm             681 non-null    float64\n",
      " 30  h16od_adjust        681 non-null    int64  \n",
      " 31  h16od_shift         681 non-null    int64  \n",
      " 32  n2_flag             681 non-null    int64  \n",
      " 33  Resistance          681 non-null    float64\n",
      " 34  DAS Temp            681 non-null    float64\n",
      " 35  Tray                681 non-null    int64  \n",
      " 36  Sample              681 non-null    int64  \n",
      " 37  Job                 681 non-null    int64  \n",
      " 38  Method              681 non-null    object \n",
      " 39  Error Code          681 non-null    int64  \n",
      " 40  Pulse Good          681 non-null    int64  \n",
      "dtypes: float64(20), int64(14), object(7)\n",
      "memory usage: 218.3+ KB\n"
     ]
    }
   ],
   "source": [
    "Data.info() #load the data type for each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34e927",
   "metadata": {},
   "source": [
    "An important column in this dataset is 'Idenitifer 1'. This is where the user programs in the name of each sample. For clarity, rename this column to 'SampleName'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18baebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.rename(columns={'Identifier 1':'SampleName'}) #Change name of Identifier 1 to SampleName'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a14a3",
   "metadata": {},
   "source": [
    "View this column now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e843beb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       DummyKD\n",
       "1                                       DummyKD\n",
       "2                                       DummyKD\n",
       "3                                       DummyKD\n",
       "4                                       DummyKD\n",
       "                         ...                   \n",
       "676                                     DummyKD\n",
       "677                                     DummyKD\n",
       "678                                     DummyKD\n",
       "679                                     DummyKD\n",
       "680                                     DummyKD\n",
       "Name: SampleName, Length: 681, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.SampleName #Load SampleName Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea4717",
   "metadata": {},
   "source": [
    "This column also has the same spacing issue that needs to be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89dead71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SampleName'] = Data['SampleName'].str.strip() #removing spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d8f1c",
   "metadata": {},
   "source": [
    "Load the column once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb7e3c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      DummyKD\n",
       "1      DummyKD\n",
       "2      DummyKD\n",
       "3      DummyKD\n",
       "4      DummyKD\n",
       "        ...   \n",
       "676    DummyKD\n",
       "677    DummyKD\n",
       "678    DummyKD\n",
       "679    DummyKD\n",
       "680    DummyKD\n",
       "Name: SampleName, Length: 681, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.SampleName #view column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b62b5c",
   "metadata": {},
   "source": [
    "Now the Sample Names are nice and neat. Now make a list of all the unique sample names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0545ee63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DummyKD', 'SuperBoil', 'Ultra_Boil', 'Dummy_Supreme_Boil',\n",
       "       'Mega_Boil', 'Prodigy_Boil', 'Exemplary_Boil', 'Elite_Boil',\n",
       "       'Real_MegaBoil', 'Supreme_Boil', 'USGS50', 'PV1158_02',\n",
       "       'PV1154_01', 'RealKD', 'DMP55', 'PV1115_03', 'PV1115_04', 'DMP_57',\n",
       "       'DMP_64', 'DMP_47', 'USGS48', 'DI_Water', 'NES9888_07',\n",
       "       'NES4448_05', 'C107_02', 'DI_Seat', 'C158_08', 'C158_09',\n",
       "       'IceLava', 'SeatW', 'C157_07', 'C157_09', 'NM2', 'MissMT',\n",
       "       'USGS47'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Samples_List=pd.unique(Data[\"SampleName\"]) #create list of unique items in the column 'SampleName'\n",
    "Samples_List #View the array of unique sample names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bcc3a2",
   "metadata": {},
   "source": [
    "Now repeat these processes for the sample types (control, standard, unknown, conditioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d61f711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Conditioning', 'Control', 'Standard', 'Unknown'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = Data.rename(columns={'Identifier 2':'SampleType'}) #Change name of Identifier 1 to SampleType'\n",
    "Data['SampleType'] = Data['SampleType'].str.strip() #removing spacing\n",
    "Sample_Type_List=pd.unique(Data[\"SampleType\"]) #create list of unique items in the column 'SampleName'\n",
    "Sample_Type_List #View the array of unique sample names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7af99b",
   "metadata": {},
   "source": [
    "Now generate a dataframe using both lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63e2fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_Type= Data.groupby(by=['SampleName', 'SampleType'], as_index=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eed18d",
   "metadata": {},
   "source": [
    "Now you can create individual lists for each type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f24f6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Standards=list(Name_Type.loc[Name_Type['SampleType'] == 'Standard', 'SampleName'])\n",
    "Controls=list(Name_Type.loc[Name_Type['SampleType'] == 'Control', 'SampleName'])\n",
    "Conditioning=list(Name_Type.loc[Name_Type['SampleType'] == 'Conditioning', 'SampleName'])\n",
    "Unknowns=list(Name_Type.loc[Name_Type['SampleType'] == 'Unknown', 'SampleName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658298e",
   "metadata": {},
   "source": [
    "Using the list of unique sample names, now a data dictionary can be created that contains the dataframes for each sample assessed during the analysis run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "485d9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Samples_Dictionary = {elem : pd.DataFrame() for elem in Samples_List} #Define data dictionary based on Sample Names array\n",
    "\n",
    "for key in Samples_Dictionary.keys():\n",
    "    Samples_Dictionary[key] = Data[:][Data.SampleName == key] \n",
    "    #generate function that adds a dataframe for each element in the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd23feb",
   "metadata": {},
   "source": [
    "Now that the data dictionary is setup, the data needs to be corrected since it contains only raw measurments. To correct data, two standards (either VSMOW and SLAP for a standards run, or in-house standards for analysis run need to be used).\n",
    "\n",
    "Before generating correction equations, the dataframes related to both standards need to be cleaned. To do this, a mean and standard deviation will be generated for each standard, and then a cutoff line will be determined using this standard deviation and mean. Measurments above and a below this cutoff line will be removed, so a number of standard deviations that is suitable for your data should be determined which will set the cutoff line (2sd, 1sd, or lower from the mean). \n",
    "\n",
    "*** EDIT THE LINES BELOW BASED ON DESIRED NUMBER OF STANDARD DEVIATIONS*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e7b7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_VALUE= 0.75  #set the # of standard deviations you would like to use to remove memory effect measurments\n",
    "\n",
    "for x in Standards:\n",
    "    mean= np.mean(Samples_Dictionary[x]['d(18_16)Mean']) #Generate mean value\n",
    "    std = np.std(Samples_Dictionary[x]['d(18_16)Mean']) #Generate standard deviation value\n",
    "    cutoff = std * STD_VALUE #Determine cutoff based on # of standard deviations desired\n",
    "    condition= ~((Samples_Dictionary[x]['d(18_16)Mean'] < (mean - cutoff)) | (Samples_Dictionary[x]['d(18_16)Mean'] > (mean + cutoff)))\n",
    "    #Create the conditions to apply\n",
    "    Clean = Samples_Dictionary[x][condition] #Apply conditions\n",
    "    Samples_Dictionary[x]=Clean #Update based on cleaned version\n",
    "    \n",
    "STD_VALUE= 1  #set the # of standard deviations you would like to use to remove outliers\n",
    "\n",
    "for x in Standards:\n",
    "    mean= np.mean(Samples_Dictionary[x]['d(18_16)Mean']) #Generate mean value\n",
    "    std = np.std(Samples_Dictionary[x]['d(18_16)Mean']) #Generate standard deviation value\n",
    "    cutoff = std * STD_VALUE #Determine cutoff based on # of standard deviations desired\n",
    "    condition= ~((Samples_Dictionary[x]['d(18_16)Mean'] < (mean - cutoff)) | (Samples_Dictionary[x]['d(18_16)Mean'] > (mean + cutoff)))\n",
    "    #Create the conditions to apply to VSMOW\n",
    "    Clean = Samples_Dictionary[x][condition] #Apply conditions to VSMOW\n",
    "    Samples_Dictionary[x]=Clean #Update VSMOW based on cleaned version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71431f9",
   "metadata": {},
   "source": [
    "*** END OF EDITING ***\n",
    "\n",
    "Now both standards have been cleaned and can be used to create correction equations. \n",
    "\n",
    "To do this, a stretching and offset value will be generated for both d17O and d18O. These equations will be used to correct both the raw d17O and d18O values obtained from the isotope analyzer instrument. \n",
    "\n",
    "E17O can then be corrected using these corrected d17O and d18O values\n",
    "\n",
    "To begin, determine the stretching value for d18O. \n",
    "\n",
    "This is determined by generating a change value for the raw measurments of both standards and by generating a change value for the known values of both standards.\n",
    "\n",
    "The known change value is then divided by the raw change value.\n",
    "\n",
    "To begin, raw and known means should be set for both standards for d17O, d18O, and dH. The known values should be set using the Values file loaded in earlier. ***BE SURE THAT THE NAMES WITHIN THE VALUES FILE MATCH THOSE FROM THE RAW DATAFILE***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69c14da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard_1=Standards[0] #set Standard 1\n",
    "Standard_2=Standards[1] #set Standard 2\n",
    "\n",
    "Standard1_d18O_Raw=np.mean(Samples_Dictionary[Standard_1]['d(18_16)Mean']) #sets raw d18O mean value\n",
    "Standard1_d17O_Raw=np.mean(Samples_Dictionary[Standard_1]['d(17_16)Mean']) #sets raw d17O mean value\n",
    "Standard1_2H_Raw=np.mean(Samples_Dictionary[Standard_1]['d(D_H)Mean']) #sets raw 2H mean value\n",
    "\n",
    "Standard2_d18O_Raw=np.mean(Samples_Dictionary[Standard_2]['d(18_16)Mean']) #sets raw d18O mean value \n",
    "Standard2_d17O_Raw=np.mean(Samples_Dictionary[Standard_2]['d(17_16)Mean']) #sets raw d17O mean value\n",
    "Standard2_2H_Raw=np.mean(Samples_Dictionary[Standard_2]['d(D_H)Mean']) #sets raw 2H mean value\n",
    "\n",
    "Standard1_d18O_Known=(pd.to_numeric(eval(Standard_1+'_d18'))) #sets known d18O value\n",
    "Standard1_d17O_Known=(pd.to_numeric(eval(Standard_1+'_d17'))) #sets known d17O value\n",
    "Standard1_2H_Known=(pd.to_numeric(eval(Standard_1+'_2H'))) #sets known 2H mean value\n",
    "Standard1_E17_Known=(pd.to_numeric(eval(Standard_1+'_E17'))) #sets known E17 mean value\n",
    "\n",
    "Standard2_d18O_Known=(pd.to_numeric(eval(Standard_2+'_d18'))) #sets known d18O value\n",
    "Standard2_d17O_Known=(pd.to_numeric(eval(Standard_2+'_d17'))) #sets known d17O value\n",
    "Standard2_2H_Known=(pd.to_numeric(eval(Standard_2+'_2H'))) #sets known 2H mean value\n",
    "Standard2_E17_Known=(pd.to_numeric(eval(Standard_2+'_E17'))) #sets known E17 mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de2bbc",
   "metadata": {},
   "source": [
    "With values for both raw and known d18O measurments set, a d18O stretching value can now be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "142afaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEASURED d18O Standard 1 - MEASURED d18O Standard 2= CHANGE_1\n",
    "d18O_Change1=Standard1_d18O_Raw-Standard2_d18O_Raw\n",
    "# KNOWN d18O Standard 1- KNOWN d18O Standard 2 = CHANGE 2\n",
    "d18O_Change2=Standard1_d18O_Known - Standard2_d18O_Known\n",
    "# Stretching factor = CHANGE_2 / CHANGE_1\n",
    "d18O_stretch=d18O_Change2/d18O_Change1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc92f7",
   "metadata": {},
   "source": [
    "Each raw value will be multiplied by this stretching value. However, an offset value still needs to be obtained. \n",
    "\n",
    "The offset is determined by subtracting the raw value * stretching value from the known value for both standards and then averaging these values together. The offset value should be nearly identical for each standard so often only one offset value is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3818655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNOWN d18O Standard 1 - (MEASURED d18O Standard 1 * Stretching factor)\n",
    "d18O_Offset_1=Standard1_d18O_Known - (Standard1_d18O_Raw * d18O_stretch)\n",
    "# Now the same for Standard 2\n",
    "d18O_Offset_2=Standard2_d18O_Known - (Standard2_d18O_Raw * d18O_stretch)\n",
    "# Finalize\n",
    "d18O_Final_Offset= (d18O_Offset_1+d18O_Offset_2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feff54",
   "metadata": {},
   "source": [
    "This offset value will be added to the product of (raw value * stretching value) \n",
    "\n",
    "As such, the final equation looks like: y = (raw value * stretching value) + offset\n",
    "\n",
    "When simplified, this is essentially just a *y=mx+b equation*, with y representing the corrected value. \n",
    "\n",
    "Define a function based on the obtained d18O stretching and offset values. This will be used to correct all raw values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6da5669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d18O_amended(x):\n",
    "    return((d18O_stretch*x)+d18O_Final_Offset) #function to correct raw d18O values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0e4e5",
   "metadata": {},
   "source": [
    "Now that a correction equation has been determined for d18O, an equation needs to be determined for d17O\n",
    "\n",
    "The format for generating a d17O correction is slightly different because d17O values are less certain except for VSMOW. \n",
    "\n",
    "Therefore, a backcalculation using d18O and E17O is applied to obtain a d17O approximate value for SLAP and other in-house standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b577e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back calculate d17O linearized\n",
    "Std1_Lin_d17O=(Standard1_E17_Known)+(0.528*(1000*np.log((Standard1_d18O_Known/1000)+1)))\n",
    "# Now transition from linearized to normal d17O\n",
    "Std1_Approx_d17O=1000*(exp(Std1_Lin_d17O/1000)-1)\n",
    "# Repeat for Standard 2\n",
    "Std2_Lin_d17O=(Standard2_E17_Known)+(0.528*(1000*np.log((Standard2_d18O_Known/1000)+1)))\n",
    "Std2_Approx_d17O=1000*(exp(Std2_Lin_d17O/1000)-1)\n",
    "# MEASURED d17O STD1 - MEASURED d17O STD 1= CHANGE_1\n",
    "d17O_Change1=Standard1_d17O_Raw-Standard2_d17O_Raw\n",
    "# KNOWN d17O STD2 - KNOWN d17O STD2 = CHANGE 2\n",
    "d17O_Change2=Std1_Approx_d17O - Std2_Approx_d17O\n",
    "# Stretching factor = CHANGE_2 / CHANGE_1\n",
    "d17O_stretch=d17O_Change2/d17O_Change1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bcc98",
   "metadata": {},
   "source": [
    "Now with a d17O stretching factor set, the same process as with the d18O values can be repeated to determine a d17O offset value and then create a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3afc823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNOWN d17O STD1 - (MEASURED d17O STD1 * Stretching factor)\n",
    "d17O_Offset_1=Std1_Approx_d17O - (Standard1_d17O_Raw * d17O_stretch)\n",
    "# Now STD2\n",
    "d17O_Offset_2=Std2_Approx_d17O - (Standard2_d17O_Raw * d17O_stretch)\n",
    "# Finalize\n",
    "d17O_Final_Offset=(d17O_Offset_1+d17O_Offset_2)/2\n",
    "# amended d17O = stretching * raw sample d17O + offset \n",
    "\n",
    "def d17O_amended(x):\n",
    "    return((d17O_stretch*x)+d17O_Final_Offset) #function to correct raw d17O values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde07823",
   "metadata": {},
   "source": [
    "Now with both a d17O and d18O correction equation function created, a E17O correction equation function can be created as well. This function uses the corrected d17O and d18O values to determine a corrected E17O value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9628ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E17O_amended(x,y):\n",
    "    return(((((1000*np.log((x/1000)+1))))-(0.528*(1000*np.log((y/1000)+1)))))\n",
    "#function to determine corrected E17O value from corrected d17O and d18O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3e383",
   "metadata": {},
   "source": [
    "Lastly, this stretching offset process is repeated for 2H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c176514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEASURED d2H Standard 1 - MEASURED d2H Standard 2= CHANGE_1\n",
    "dH_Change1=Standard1_2H_Raw-Standard2_2H_Raw\n",
    "# KNOWN d2H Standard 1- KNOWN d2H Standard 2 = CHANGE 2\n",
    "dH_Change2=Standard1_2H_Known - Standard2_2H_Known\n",
    "# Stretching factor = CHANGE_2 / CHANGE_1\n",
    "dH_stretch=dH_Change2/dH_Change1\n",
    "\n",
    "# KNOWN d2H Standard 1 - (MEASURED d2H Standard 1 * Stretching factor)\n",
    "dH_Offset_1=Standard1_2H_Known - (Standard1_2H_Raw * dH_stretch)\n",
    "# Now the same for Standard 2\n",
    "dH_Offset_2=Standard2_2H_Known - (Standard2_2H_Raw * dH_stretch)\n",
    "# Finalize\n",
    "dH_Final_Offset= (dH_Offset_1+dH_Offset_2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16cf00",
   "metadata": {},
   "source": [
    "With a stretching offset determined for 2H, a corrected d2H can be obtained that can then be combined with the corrected d18O to generate a d-excess correction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6507ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dH_amended(x):\n",
    "    return((dH_stretch*x)+dH_Final_Offset) #function to correct raw d2H values\n",
    "\n",
    "def d_excess(x,y):\n",
    "    return(x-8*y) #function to caculate d-excess values from amended d2H and d18O values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe2732",
   "metadata": {},
   "source": [
    "Now that all functions have been generated, we can apply these functions to all measurments to correct d2H, d18O, and d17O values. \n",
    "\n",
    "After correcting these values, we can then use the corrected values for the d-excess and E17O correction functions.\n",
    "\n",
    "The following function applies all the correction quations to each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3be1525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in Samples_List:\n",
    "    Samples_Dictionary[x]['d18O_amended']=d18O_amended(Samples_Dictionary[x]['d(18_16)Mean'])\n",
    "# apply d18O correction function to all raw d18O values in data dictionary\n",
    "\n",
    "for x in Samples_List:\n",
    "    Samples_Dictionary[x]['d17O_amended']=d17O_amended(Samples_Dictionary[x]['d(17_16)Mean'])\n",
    "# apply d17O correction function to all raw d17O values in data dictionary\n",
    "\n",
    "for x in Samples_List:\n",
    "    Samples_Dictionary[x]['E17O_amended']=E17O_amended(Samples_Dictionary[x]['d17O_amended'], Samples_Dictionary[x]['d18O_amended'])\n",
    "# apply E17O correction function using corrected d17O and d18O\n",
    "    \n",
    "for x in Samples_List:\n",
    "    Samples_Dictionary[x]['dH_amended']=dH_amended(Samples_Dictionary[x]['d(D_H)Mean'])\n",
    "# apply d2H correction function to all raw d2H values in data dictionary\n",
    "\n",
    "for x in Samples_List:\n",
    "    Samples_Dictionary[x]['d_excess']=d_excess(Samples_Dictionary[x]['dH_amended'], Samples_Dictionary[x]['d18O_amended'])\n",
    "# apply d-excess correction function using corrected d2H and d18O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7276834",
   "metadata": {},
   "source": [
    "At this point, we should remove the coniditioning vials since they are not being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b378e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in Conditioning: \n",
    "    del Samples_Dictionary[x] #removes all conditioning samples from the data dictionary\n",
    "Final_Samples_List = list(Samples_Dictionary) #generates new list without the conditioning vials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a866b",
   "metadata": {},
   "source": [
    "At this point it's important to check E17O values to make sure they look accurate and standard deviations aren't too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61e0032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elite_Boil -0.025644784431560038\n",
      "Real_MegaBoil -0.008575400501343118\n",
      "Supreme_Boil -0.0014521424273732173\n",
      "USGS50 -0.0025926790370085937\n",
      "PV1158_02 -0.11111201475699856\n",
      "PV1154_01 -0.10806399196407193\n",
      "RealKD 0.011817347152973666\n",
      "DMP55 -0.10674119085571533\n",
      "PV1115_03 -0.11336690233189153\n",
      "PV1115_04 -0.1112109874332347\n",
      "DMP_57 -0.1085845181388282\n",
      "DMP_64 -0.10153883304309706\n",
      "DMP_47 -0.09019795837274036\n",
      "USGS48 0.06227499830314964\n",
      "DI_Water 0.047936638544486675\n",
      "NES9888_07 -0.05018173585032192\n",
      "NES4448_05 -0.06344961256572727\n",
      "C107_02 -0.0656048332276162\n",
      "DI_Seat 0.05192947796132158\n",
      "C158_08 -0.0564451983684642\n",
      "C158_09 -0.05502950780982236\n",
      "IceLava 0.056480644184657376\n",
      "SeatW 0.042361149178085894\n",
      "C157_07 -0.028801700010605023\n",
      "C157_09 -0.06959294761422778\n",
      "NM2 0.012465285491023272\n",
      "MissMT 0.03457613051209831\n",
      "USGS47 0.035371927182815295\n"
     ]
    }
   ],
   "source": [
    "for x in Final_Samples_List:\n",
    "    print(x,np.mean(Samples_Dictionary[x]['E17O_amended'])) #function to look at all samples E17O value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dde830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elite_Boil 0.01716191624413617\n",
      "Real_MegaBoil 0.014979172106259361\n",
      "Supreme_Boil 0.012954185703685507\n",
      "USGS50 0.013806556730722515\n",
      "PV1158_02 0.0126042196207676\n",
      "PV1154_01 0.006240246644150637\n",
      "RealKD 0.019112141148430597\n",
      "DMP55 0.01759501593253339\n",
      "PV1115_03 0.012438039099465796\n",
      "PV1115_04 0.019136319802319262\n",
      "DMP_57 0.014275351543787245\n",
      "DMP_64 0.014741728287895325\n",
      "DMP_47 0.013620177258895581\n",
      "USGS48 0.014957627253242714\n",
      "DI_Water 0.011878793152484005\n",
      "NES9888_07 0.010594035676108263\n",
      "NES4448_05 0.013490726704174248\n",
      "C107_02 0.013764099437712183\n",
      "DI_Seat 0.014371350142292957\n",
      "C158_08 0.012446132255202642\n",
      "C158_09 0.014275148595382352\n",
      "IceLava 0.012987559229861912\n",
      "SeatW 0.011793996566177523\n",
      "C157_07 0.0172332327098604\n",
      "C157_09 0.011203063276927465\n",
      "NM2 0.01204840694449677\n",
      "MissMT 0.015642591195374308\n",
      "USGS47 0.013047729851194621\n"
     ]
    }
   ],
   "source": [
    "for x in Final_Samples_List:\n",
    "    print(x,np.std(Samples_Dictionary[x]['E17O_amended']))  #function to look at all samples E17O standard deviation value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e8d53",
   "metadata": {},
   "source": [
    "If E17O values and standard deviations look correct, then at this point individual csv files should be exported for each sample before proceeding to make further adjustments.\n",
    "\n",
    "This is critical because many columns will be removed in the proceeding steps and it may be necessary to examine these columns down the road.\n",
    "\n",
    "*** EDIT THE FOLLOWING LINES BASED ON THE LOCATION YOU'D LIKE TO EXPORT CSV FILES ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a5369ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in Final_Samples_List:\n",
    "    Samples_Dictionary[x].to_csv('..\\\\Automated_Templates\\\\'+str(x)+timestr+'.csv', sep=',') \n",
    "    #function that exports each sample data as an individual dataframe with the date in day_year_month format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293914b",
   "metadata": {},
   "source": [
    "*** END OF EDITING ***\n",
    "\n",
    "At this point, only the 2 standards have been cleaned. All the other samples should go through the same cutoff process described above based based on their means,standard deviations, and determined cutoff lines. This process slightly differs because now the E17O value is used instead of the d18O value.\n",
    "\n",
    "Based on the dataset, a number of standard deviations for the cutoff line should be set for both the control and unknownn samples. Typically since unknown samples have fewer measurments, the number of standadard deviations used for the cutoff should be more relaxed than for the controls and standards.\n",
    "\n",
    "First, the control samples will be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8797ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_VALUE= 0.75  #set the # of standard deviations you would like to use to remove memory effect measurments\n",
    "\n",
    "for x in Controls:\n",
    "    mean= np.mean(Samples_Dictionary[x]['E17O_amended']) #Generate mean value\n",
    "    std = np.std(Samples_Dictionary[x]['E17O_amended']) #Generate standard deviation value\n",
    "    cutoff = std * STD_VALUE #Determine cutoff based on # of standard deviations desired\n",
    "    condition= ~((Samples_Dictionary[x]['E17O_amended'] < (mean - cutoff)) | (Samples_Dictionary[x]['E17O_amended'] > (mean + cutoff)))\n",
    "    #Create the conditions to apply\n",
    "    Clean = Samples_Dictionary[x][condition] #Apply conditions\n",
    "    Samples_Dictionary[x]=Clean #Update based on cleaned version\n",
    "    \n",
    "STD_VALUE= 1  #set the # of standard deviations you would like to use to remove outliers\n",
    "\n",
    "for x in Controls:\n",
    "    mean= np.mean(Samples_Dictionary[x]['E17O_amended']) #Generate mean value\n",
    "    std = np.std(Samples_Dictionary[x]['E17O_amended']) #Generate standard deviation value\n",
    "    cutoff = std * STD_VALUE #Determine cutoff based on # of standard deviations desired\n",
    "    condition= ~((Samples_Dictionary[x]['E17O_amended'] < (mean - cutoff)) | (Samples_Dictionary[x]['E17O_amended'] > (mean + cutoff)))\n",
    "    #Create the conditions to apply to VSMOW\n",
    "    Clean = Samples_Dictionary[x][condition] #Apply conditions to VSMOW\n",
    "    Samples_Dictionary[x]=Clean #Update VSMOW based on cleaned version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d51b6",
   "metadata": {},
   "source": [
    "Next the unknown samples will be cleaned. As mentioned prior, since fewer measuremnts are made for the unknowns this process will simply be repeated for both the memory effect and outliers using the same STD value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02646caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove memory effect measurements for unknown samples and outliers.\n",
    "\n",
    "STD_VALUE= 1  #set number of standard deviations to remove memory effect measurments and outliers\n",
    "\n",
    "for x in Unknowns:\n",
    "    for i in range(2):\n",
    "        mean= np.mean(Samples_Dictionary[x]['E17O_amended'])\n",
    "        std = np.std(Samples_Dictionary[x]['E17O_amended'])\n",
    "        cutoff = std * STD_VALUE\n",
    "        condition= ~((Samples_Dictionary[x]['E17O_amended'] < (mean - cutoff)) | (Samples_Dictionary[x]['E17O_amended'] > (mean + cutoff)))\n",
    "        Clean = Samples_Dictionary[x][condition]\n",
    "        Samples_Dictionary[x]=Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf557b",
   "metadata": {},
   "source": [
    "Again, it's important to pause here now to review the updated mean E17O values and standard deviations now that the datasets have been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "935130e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elite_Boil -0.022722440772339616\n",
      "Real_MegaBoil -0.006861087586654158\n",
      "Supreme_Boil -1.7514029481484933e-05\n",
      "USGS50 -0.0025926790370085937\n",
      "PV1158_02 -0.10920102640400153\n",
      "PV1154_01 -0.10594621221621237\n",
      "RealKD 0.015002241061779192\n",
      "DMP55 -0.10636077972944885\n",
      "PV1115_03 -0.11143851642226305\n",
      "PV1115_04 -0.10922452833699359\n",
      "DMP_57 -0.11659376624893807\n",
      "DMP_64 -0.10192862468339048\n",
      "DMP_47 -0.09298930710070774\n",
      "USGS48 0.061939883211746954\n",
      "DI_Water 0.04646869724857239\n",
      "NES9888_07 -0.0479037156199702\n",
      "NES4448_05 -0.06822987431299003\n",
      "C107_02 -0.06307920745555966\n",
      "DI_Seat 0.0506520423839355\n",
      "C158_08 -0.049260111484320634\n",
      "C158_09 -0.055122340907277856\n",
      "IceLava 0.056413895039441275\n",
      "SeatW 0.043328478769644185\n",
      "C157_07 -0.02746694076363602\n",
      "C157_09 -0.06875725993744315\n",
      "NM2 0.009437771593017667\n",
      "MissMT 0.034341009797403524\n",
      "USGS47 0.035371927182815295\n"
     ]
    }
   ],
   "source": [
    "for x in Final_Samples_List:\n",
    "    print(x,np.mean(Samples_Dictionary[x]['E17O_amended'])) #function to view E17O mean values for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd3c4e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elite_Boil 0.0038090240529099727\n",
      "Real_MegaBoil 0.00413992067443756\n",
      "Supreme_Boil 0.0034078853187832774\n",
      "USGS50 0.013806556730722515\n",
      "PV1158_02 0.0031203261742830963\n",
      "PV1154_01 0.0016927791987259918\n",
      "RealKD 0.006667831652818057\n",
      "DMP55 0.00495463934957489\n",
      "PV1115_03 0.004589714936967674\n",
      "PV1115_04 0.007951092646773291\n",
      "DMP_57 0.0037804193038390075\n",
      "DMP_64 0.004110419540938649\n",
      "DMP_47 0.0032004385171510233\n",
      "USGS48 0.004029971608290253\n",
      "DI_Water 0.0027219922141496033\n",
      "NES9888_07 0.0029772534249707535\n",
      "NES4448_05 0.003190090502723849\n",
      "C107_02 0.00492748023548843\n",
      "DI_Seat 0.003828940188712073\n",
      "C158_08 0.0042653893706056396\n",
      "C158_09 0.00225011948188792\n",
      "IceLava 0.002496158164890925\n",
      "SeatW 0.001514143118505931\n",
      "C157_07 0.004851723556471259\n",
      "C157_09 0.0035335466025242356\n",
      "NM2 0.0016611904616314684\n",
      "MissMT 0.004445245211706582\n",
      "USGS47 0.013047729851194621\n"
     ]
    }
   ],
   "source": [
    "for x in Final_Samples_List:\n",
    "    print(x,np.std(Samples_Dictionary[x]['E17O_amended'])) #function to view standard deviation values for all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ede574",
   "metadata": {},
   "source": [
    "Now proceed to finalizing the datasets so that a final spreadsheet can be exported. This involves removing excess columns and spacing the data out so that it is easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ee073a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in Final_Samples_List:\n",
    "    Samples_Dictionary[x]['_']= \"\" #Add blank column\n",
    "    Samples_Dictionary[x]['__']= \"\" #Add blank column\n",
    "    Samples_Dictionary[x]['___']= \"\" #Add blank column\n",
    "    Samples_Dictionary[x]['____']= \"\" #Add blank column\n",
    "    Samples_Dictionary[x]['_____']= \"\" #Add blank column\n",
    "    Samples_Dictionary[x]=Samples_Dictionary[x][['Inj Nr','_','d(17_16)Mean','d17O_amended','__',\n",
    "                                                       'd(18_16)Mean','d18O_amended','___','d(D_H)Mean','dH_amended','d_excess','____',\n",
    "                                                       'E17_Mean','E17O_amended','_____',\"H2O_Mean\"]].copy()\n",
    "    #This last portion of the function creates a new version of the data dictionary with only the necessary columns and \n",
    "    #adds spaces between sections to make the dataframes easier to read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06467b73",
   "metadata": {},
   "source": [
    "Controls and standards will be finalized in a similar manner because both of these have known/established values to compare to. In comparison, unknown values do not have anything to compare with, so only a mean and standard deviation will be generated for these samples.\n",
    "\n",
    "Begin with finalizing the controls and standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "645ff3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize control data\n",
    "for x in Controls:\n",
    "    #Create new row based on the mean values\n",
    "    row_1=['','','',np.mean(Samples_Dictionary[x]['d17O_amended']),'','',\n",
    "           np.mean(Samples_Dictionary[x]['d18O_amended']),'','',np.mean(Samples_Dictionary[x]['dH_amended']),\n",
    "           np.mean(Samples_Dictionary[x]['d_excess']),'','',np.mean(Samples_Dictionary[x]['E17O_amended']),'',\n",
    "           np.mean(Samples_Dictionary[x]['H2O_Mean'])]\n",
    "    #Create new row based on the standard deviations\n",
    "    row_2=['','','',np.std(Samples_Dictionary[x]['d17O_amended']),'','',np.std(Samples_Dictionary[x]['d18O_amended']),'','',\n",
    "           np.std(Samples_Dictionary[x]['dH_amended']),np.mean(Samples_Dictionary[x]['d_excess']),\n",
    "           '','',np.std(Samples_Dictionary[x]['E17O_amended']),'',np.std(Samples_Dictionary[x]['H2O_Mean'])]\n",
    "    #Create new row based on the difference between measured and know/established values\n",
    "    row_3=['','','',(pd.to_numeric(eval(x+'_d17')) - np.mean(Samples_Dictionary[x]['d17O_amended'])),'','',\n",
    "        (pd.to_numeric(eval(x+'_d18')) - np.mean(Samples_Dictionary[x]['d18O_amended'])),'','',\n",
    "        (pd.to_numeric(eval(x+'_2H')) - np.mean(Samples_Dictionary[x]['dH_amended'])), \n",
    "        (pd.to_numeric(eval(x+'_D_Excess')) - np.mean(Samples_Dictionary[x]['d_excess'])),\n",
    "        '','',(pd.to_numeric(eval(x+'_E17')) - np.mean(Samples_Dictionary[x]['E17O_amended'])),'','',]\n",
    "    Samples_Dictionary[x].loc['MEAN']=row_1 #add row 1 to end of dataframe\n",
    "    Samples_Dictionary[x].loc['STD']=row_2 #add row 2 to end of dataframe\n",
    "    Samples_Dictionary[x].loc['DIFFERENCE FROM KNOWN/ESTABLISHED']=row_3 #add row 3 to end of dataframe\n",
    "\n",
    "# Now repeat this same process for the standards\n",
    "    \n",
    "# Finalize Standard data\n",
    "for x in Standards:\n",
    "    #Create new row based on the mean values\n",
    "    row_1=['','','',np.mean(Samples_Dictionary[x]['d17O_amended']),'','',\n",
    "           np.mean(Samples_Dictionary[x]['d18O_amended']),'','',np.mean(Samples_Dictionary[x]['dH_amended']),\n",
    "           np.mean(Samples_Dictionary[x]['d_excess']),'','',np.mean(Samples_Dictionary[x]['E17O_amended']),'',\n",
    "           np.mean(Samples_Dictionary[x]['H2O_Mean'])]\n",
    "    #Create new row based on the standard deviations\n",
    "    row_2=['','','',np.std(Samples_Dictionary[x]['d17O_amended']),'','',np.std(Samples_Dictionary[x]['d18O_amended']),'','',\n",
    "           np.std(Samples_Dictionary[x]['dH_amended']),np.mean(Samples_Dictionary[x]['d_excess']),\n",
    "           '','',np.std(Samples_Dictionary[x]['E17O_amended']),'',np.std(Samples_Dictionary[x]['H2O_Mean'])]\n",
    "    #Create new row based on the difference between measured and know/established values\n",
    "    row_3=['','','',(pd.to_numeric(eval(x+'_d17')) - np.mean(Samples_Dictionary[x]['d17O_amended'])),'','',\n",
    "        (pd.to_numeric(eval(x+'_d18')) - np.mean(Samples_Dictionary[x]['d18O_amended'])),'','',\n",
    "        (pd.to_numeric(eval(x+'_2H')) - np.mean(Samples_Dictionary[x]['dH_amended'])), \n",
    "        (pd.to_numeric(eval(x+'_D_Excess')) - np.mean(Samples_Dictionary[x]['d_excess'])),\n",
    "        '','',(pd.to_numeric(eval(x+'_E17')) - np.mean(Samples_Dictionary[x]['E17O_amended'])),'','',]\n",
    "    Samples_Dictionary[x].loc['MEAN']=row_1 #add row 1 to end of dataframe\n",
    "    Samples_Dictionary[x].loc['STD']=row_2 #add row 2 to end of dataframe\n",
    "    Samples_Dictionary[x].loc['DIFFERENCE FROM KNOWN/ESTABLISHED']=row_3 #add row 3 to end of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8adcb",
   "metadata": {},
   "source": [
    "Now proceed to finalizing the unknown samples. Again, the difference here is that there will be no row (row_3) for the difference between the known/established value because these are unknonws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b16539fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize Unknowns\n",
    "for x in Unknowns:\n",
    "    #Create new row based on the mean values\n",
    "    row_1=['','','',np.mean(Samples_Dictionary[x]['d17O_amended']),'','',\n",
    "           np.mean(Samples_Dictionary[x]['d18O_amended']),'','',np.mean(Samples_Dictionary[x]['dH_amended']),\n",
    "           np.mean(Samples_Dictionary[x]['d_excess']),'','',np.mean(Samples_Dictionary[x]['E17O_amended']),'',\n",
    "           np.mean(Samples_Dictionary[x]['H2O_Mean'])]\n",
    "    #Create new row based on the standard deviations\n",
    "    row_2=['','','',np.std(Samples_Dictionary[x]['d17O_amended']),'','',np.std(Samples_Dictionary[x]['d18O_amended']),'','',\n",
    "           np.std(Samples_Dictionary[x]['dH_amended']),np.mean(Samples_Dictionary[x]['d_excess']),\n",
    "           '','',np.std(Samples_Dictionary[x]['E17O_amended']),'',np.std(Samples_Dictionary[x]['H2O_Mean'])]\n",
    "    Samples_Dictionary[x].loc['MEAN']=row_1 #add row 1 to end of dataframe\n",
    "    Samples_Dictionary[x].loc['STD']=row_2 #add row 2 to end of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9c72e",
   "metadata": {},
   "source": [
    "Now the last step is to export these dataframes as one spreadsheet with each unique sample in their own sheet. This is accomplished using the openxlsx package loaded in earlier\n",
    "\n",
    "*** EDIT FOLLOWING LINES TO DICTATE WHERE TO EXPORT XLSX FILE TO ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe94b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('..\\\\Automated_Templates\\\\Unknowns_'+timestr+'.xlsx') as writer:\n",
    "    for x in Final_Samples_List:\n",
    "        Samples_Dictionary[x].to_excel(writer, sheet_name=\"\"+str(x)+\"\")\n",
    "        # Generates an excel spreadsheet in the specified location and adds a sheet for each sample "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
